{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "import re \n",
    "import pickle\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnamed columns which are not necessary\n",
    "# sample_data=pd.read_csv('final_dataset.csv',encoding='latin1')\n",
    "# sample_data.drop(sample_data.columns[sample_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "# sample_data.head(15)\n",
    "# sample_data.to_csv(\"final_datasetv2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>verified</th>\n",
       "      <th>tweets</th>\n",
       "      <th>location</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/pokimanelol</td>\n",
       "      <td>content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@Nihaachu the absolute cutiest patootiest Ã°Â...</td>\n",
       "      <td>cali Ã¢ÂÂÃ¯Â¸Â</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/neekolul</td>\n",
       "      <td>Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...</td>\n",
       "      <td>33333@brodinplett Sir u worked on the set of e...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/ValorLeaks</td>\n",
       "      <td>Valorant Content Creator &amp; Influencer | Valora...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Here's mine. https://t.co/CI9DCGH8fkWhat did y...</td>\n",
       "      <td>in a pitt</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/SuperSaf</td>\n",
       "      <td>Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Looking forward to the #NextAtAcer Global Pres...</td>\n",
       "      <td>Leicester, UK</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/NyanNyanners</td>\n",
       "      <td>horrible creature\\n| mamas: @muryou_tada @Nia_...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@ironmouse @BubiVT Honestly true lmfao@_cherry...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://twitter.com/FurqanShayk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@AribaShahid @CareemPAK Yo safety first, even ...</td>\n",
       "      <td>copenhagen / islamabad</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://twitter.com/RiotCreatorSupp</td>\n",
       "      <td>North American Influencer Team @RiotGames supp...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>. #RiotGrandPrix is back. Right now - see who ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://twitter.com/HappyPower</td>\n",
       "      <td>Content Creator for @MisfitsGG, News Reporter ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>OG Leaking Flash Back https://t.co/ujDj47AHsn@...</td>\n",
       "      <td>Use Code ALTMARZ In the Item Shop!</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://twitter.com/elgato</td>\n",
       "      <td>Stream. Record. Create. | Empowering content c...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@ItzNefarious Yes! \\n\\nÃ°ÂÂÂ https://t.co/u...</td>\n",
       "      <td>Munich, Germany</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://twitter.com/Vikkstar123</td>\n",
       "      <td>Content Creator Ã¢ÂÂ¢ Business Contact: vikba...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@joinsideplus yeetEL SIDEMEN Ã°ÂÂ«Â¡ https://...</td>\n",
       "      <td>London</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://twitter.com/EXXXOTICA</td>\n",
       "      <td>The Largest Event In The US Dedicated To Love ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>New EXXXOTICA Blog! Breaking Into The Biz Ã¢Â...</td>\n",
       "      <td>Ã¢ÂÂ",
       " CHI Ã¢ÂÂ",
       " MIA Ã¢ÂÂ",
       " NJ Ã¢ÂÂ",
       " DC</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://twitter.com/ManyVids</td>\n",
       "      <td>Monetize Your Passions Ã¢ÂÂ¨\\nNeed help? Cont...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@TJ57376027 Ours too Ã°ÂÂÂ@sloansmoans See ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://twitter.com/Nihaachu</td>\n",
       "      <td>work hard, be nice | Business Inquiries: nihac...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@Sneegsnag Nah IÃ¢ÂÂm making my own https://...</td>\n",
       "      <td>She/her</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://twitter.com/Amouranth</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>@Valkyrae Beat me up pls Ã°ÂÂÂ³Ã°ÂÂÂ http...</td>\n",
       "      <td>Ã°ÂÂÂÃ°ÂÂÂ»Ã°ÂÂÂÃ°ÂÂÂ»</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://twitter.com/DavidHenrie</td>\n",
       "      <td>Trying to bring about goodness through being a...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>My first ever job haha!!!!! https://t.co/KWcNn...</td>\n",
       "      <td>Where in the world are you?</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    url  \\\n",
       "0       https://twitter.com/pokimanelol   \n",
       "1          https://twitter.com/neekolul   \n",
       "2        https://twitter.com/ValorLeaks   \n",
       "3          https://twitter.com/SuperSaf   \n",
       "4      https://twitter.com/NyanNyanners   \n",
       "5       https://twitter.com/FurqanShayk   \n",
       "6   https://twitter.com/RiotCreatorSupp   \n",
       "7        https://twitter.com/HappyPower   \n",
       "8            https://twitter.com/elgato   \n",
       "9       https://twitter.com/Vikkstar123   \n",
       "10        https://twitter.com/EXXXOTICA   \n",
       "11         https://twitter.com/ManyVids   \n",
       "12         https://twitter.com/Nihaachu   \n",
       "13        https://twitter.com/Amouranth   \n",
       "14      https://twitter.com/DavidHenrie   \n",
       "\n",
       "                                          description verified  \\\n",
       "0   content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...     TRUE   \n",
       "1   Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...     TRUE   \n",
       "2   Valorant Content Creator & Influencer | Valora...    FALSE   \n",
       "3   Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...     TRUE   \n",
       "4   horrible creature\\n| mamas: @muryou_tada @Nia_...     TRUE   \n",
       "5                                                 NaN     TRUE   \n",
       "6   North American Influencer Team @RiotGames supp...     TRUE   \n",
       "7   Content Creator for @MisfitsGG, News Reporter ...     TRUE   \n",
       "8   Stream. Record. Create. | Empowering content c...     TRUE   \n",
       "9   Content Creator Ã¢ÂÂ¢ Business Contact: vikba...     TRUE   \n",
       "10  The Largest Event In The US Dedicated To Love ...     TRUE   \n",
       "11  Monetize Your Passions Ã¢ÂÂ¨\\nNeed help? Cont...     TRUE   \n",
       "12  work hard, be nice | Business Inquiries: nihac...     TRUE   \n",
       "13                                    Content Creator     TRUE   \n",
       "14  Trying to bring about goodness through being a...     TRUE   \n",
       "\n",
       "                                               tweets  \\\n",
       "0   @Nihaachu the absolute cutiest patootiest Ã°Â...   \n",
       "1   @sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...   \n",
       "2   Here's mine. https://t.co/CI9DCGH8fkWhat did y...   \n",
       "3   Looking forward to the #NextAtAcer Global Pres...   \n",
       "4   @ironmouse @BubiVT Honestly true lmfao@_cherry...   \n",
       "5   @AribaShahid @CareemPAK Yo safety first, even ...   \n",
       "6   . #RiotGrandPrix is back. Right now - see who ...   \n",
       "7   OG Leaking Flash Back https://t.co/ujDj47AHsn@...   \n",
       "8   @ItzNefarious Yes! \\n\\nÃ°ÂÂÂ https://t.co/u...   \n",
       "9   @joinsideplus yeetEL SIDEMEN Ã°ÂÂ«Â¡ https://...   \n",
       "10  New EXXXOTICA Blog! Breaking Into The Biz Ã¢Â...   \n",
       "11  @TJ57376027 Ours too Ã°ÂÂÂ@sloansmoans See ...   \n",
       "12  @Sneegsnag Nah IÃ¢ÂÂm making my own https://...   \n",
       "13  @Valkyrae Beat me up pls Ã°ÂÂÂ³Ã°ÂÂÂ http...   \n",
       "14  My first ever job haha!!!!! https://t.co/KWcNn...   \n",
       "\n",
       "                                             location            label  \n",
       "0                                   cali Ã¢ÂÂÃ¯Â¸Â  Content Creator  \n",
       "1   33333@brodinplett Sir u worked on the set of e...  Content Creator  \n",
       "2                                           in a pitt  Content Creator  \n",
       "3                                       Leicester, UK  Content Creator  \n",
       "4                                       United States  Content Creator  \n",
       "5                              copenhagen / islamabad  Content Creator  \n",
       "6                                                 NaN  Content Creator  \n",
       "7                  Use Code ALTMARZ In the Item Shop!  Content Creator  \n",
       "8                                     Munich, Germany  Content Creator  \n",
       "9                                              London  Content Creator  \n",
       "10          Ã¢ÂÂ\n",
       " CHI Ã¢ÂÂ\n",
       " MIA Ã¢ÂÂ\n",
       " NJ Ã¢ÂÂ\n",
       " DC  Content Creator  \n",
       "11                                                NaN  Content Creator  \n",
       "12                                            She/her  Content Creator  \n",
       "13                   Ã°ÂÂÂÃ°ÂÂÂ»Ã°ÂÂÂÃ°ÂÂÂ»  Content Creator  \n",
       "14                        Where in the world are you?  Content Creator  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading correct dataset\n",
    "sample_data=pd.read_csv('final_datasetv2.csv',encoding='latin1')\n",
    "sample_data.drop(sample_data.columns[sample_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "sample_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlook to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url            5962\n",
       "description    5945\n",
       "verified       5962\n",
       "tweets         5957\n",
       "location       4896\n",
       "label          5962\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return count of every row\n",
    "sample_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...</td>\n",
       "      <td>@Nihaachu the absolute cutiest patootiest Ã°Â...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...</td>\n",
       "      <td>@sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valorant Content Creator &amp; Influencer | Valora...</td>\n",
       "      <td>Here's mine. https://t.co/CI9DCGH8fkWhat did y...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...</td>\n",
       "      <td>Looking forward to the #NextAtAcer Global Pres...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horrible creature\\n| mamas: @muryou_tada @Nia_...</td>\n",
       "      <td>@ironmouse @BubiVT Honestly true lmfao@_cherry...</td>\n",
       "      <td>Content Creator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...   \n",
       "1  Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...   \n",
       "2  Valorant Content Creator & Influencer | Valora...   \n",
       "3  Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...   \n",
       "4  horrible creature\\n| mamas: @muryou_tada @Nia_...   \n",
       "\n",
       "                                              tweets            label  \n",
       "0  @Nihaachu the absolute cutiest patootiest Ã°Â...  Content Creator  \n",
       "1  @sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...  Content Creator  \n",
       "2  Here's mine. https://t.co/CI9DCGH8fkWhat did y...  Content Creator  \n",
       "3  Looking forward to the #NextAtAcer Global Pres...  Content Creator  \n",
       "4  @ironmouse @BubiVT Honestly true lmfao@_cherry...  Content Creator  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unnecessary columns\n",
    "sample_data = sample_data.drop(['url', 'verified', 'location'], axis=1)\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description    17\n",
       "tweets          5\n",
       "label           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null Values\n",
    "sample_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Null Values\n",
    "sample_data.dropna(axis=0,inplace=True,)\n",
    "sample_data = sample_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sports             1027\n",
       "Actor               989\n",
       "Politician          985\n",
       "Content Creator     980\n",
       "Singer              980\n",
       "Education           979\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of values\n",
    "sample_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding of the output labels\n",
    "label = pd.DataFrame({\n",
    "    \"label\": [\"Content Creator\", \"Education\", \"Actor\", \"Politician\", \"Singer\", \"Sports\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the original dataset to manipulate data without intrupting original data\n",
    "sample_data_output_encoded = sample_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.ravel(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Output Attribute After Label Encoding:\n",
      "========================================\n",
      "\n",
      "                label  Encoded_label\n",
      "0     Content Creator              1\n",
      "1     Content Creator              1\n",
      "2     Content Creator              1\n",
      "3     Content Creator              1\n",
      "4     Content Creator              1\n",
      "...               ...            ...\n",
      "5935           Sports              5\n",
      "5936           Sports              5\n",
      "5937           Sports              5\n",
      "5938           Sports              5\n",
      "5939           Sports              5\n",
      "\n",
      "[5940 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Label Encoding\n",
    "print(\"\\n\\nOutput Attribute After Label Encoding:\")\n",
    "print(\"========================================\\n\")\n",
    "sample_data[\"Encoded_label\"] = label_encoder.transform(\n",
    "    sample_data['label'])\n",
    "print(sample_data[[\"label\", \"Encoded_label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "      <th>Encoded_label</th>\n",
       "      <th>Processed description</th>\n",
       "      <th>Processed tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...</td>\n",
       "      <td>@Nihaachu the absolute cutiest patootiest Ã°Â...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>content creator instagramcompokimanelol contac...</td>\n",
       "      <td>absolut cutiest patootiest stream beta year we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...</td>\n",
       "      <td>@sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>partner spanglish streamer anim food videogam ...</td>\n",
       "      <td>benito teja todava hello qweenlt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valorant Content Creator &amp; Influencer | Valora...</td>\n",
       "      <td>Here's mine. https://t.co/CI9DCGH8fkWhat did y...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>valor content creator influenc valor datamin i...</td>\n",
       "      <td>here mine httpstcocidcghfkwhat your night mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...</td>\n",
       "      <td>Looking forward to the #NextAtAcer Global Pres...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>content creator #tech #travel #meme podcast host</td>\n",
       "      <td>look forward #nextatac global press confer liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horrible creature\\n| mamas: @muryou_tada @Nia_...</td>\n",
       "      <td>@ironmouse @BubiVT Honestly true lmfao@_cherry...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>horribl creatur mama banner #nyanart content c...</td>\n",
       "      <td>honestli true lmfao thank much thi comm seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>North American Influencer Team @RiotGames supp...</td>\n",
       "      <td>. #RiotGrandPrix is back. Right now - see who ...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>north american influenc team support content c...</td>\n",
       "      <td>#riotgrandprix back right win bracket master d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Content Creator for @MisfitsGG, News Reporter ...</td>\n",
       "      <td>OG Leaking Flash Back https://t.co/ujDj47AHsn@...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>content creator new report content creator #st...</td>\n",
       "      <td>leak flash back httpstcoujdjahsn want fortnit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stream. Record. Create. | Empowering content c...</td>\n",
       "      <td>@ItzNefarious Yes! \\n\\nÃ°ÂÂÂ https://t.co/u...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>stream record creat empow content creator sinc...</td>\n",
       "      <td>httpstcouaejpsmye httpstcoklifmleei enhanc her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Content Creator Ã¢ÂÂ¢ Business Contact: vikba...</td>\n",
       "      <td>@joinsideplus yeetEL SIDEMEN Ã°ÂÂ«Â¡ https://...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>content creator busi contact vikbarncom instag...</td>\n",
       "      <td>yeetel sidemen httpstcoonshewhgleav like https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Largest Event In The US Dedicated To Love ...</td>\n",
       "      <td>New EXXXOTICA Blog! Breaking Into The Biz Ã¢Â...</td>\n",
       "      <td>Content Creator</td>\n",
       "      <td>1</td>\n",
       "      <td>largest event dedic love wildli illprepar onli...</td>\n",
       "      <td>exxxotica blog break into howto guid get into ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  content creator Ã¢ÂÂºÃ¯Â¸Â instagram.com/pok...   \n",
       "1  Partnered Spanglish @Twitch Streamer Ã°ÂÂÂ²Ã...   \n",
       "2  Valorant Content Creator & Influencer | Valora...   \n",
       "3  Ã¢ÂÂ¶Ã¯Â¸Â Content Creator #Tech | #Travel |...   \n",
       "4  horrible creature\\n| mamas: @muryou_tada @Nia_...   \n",
       "5  North American Influencer Team @RiotGames supp...   \n",
       "6  Content Creator for @MisfitsGG, News Reporter ...   \n",
       "7  Stream. Record. Create. | Empowering content c...   \n",
       "8  Content Creator Ã¢ÂÂ¢ Business Contact: vikba...   \n",
       "9  The Largest Event In The US Dedicated To Love ...   \n",
       "\n",
       "                                              tweets            label  \\\n",
       "0  @Nihaachu the absolute cutiest patootiest Ã°Â...  Content Creator   \n",
       "1  @sanbenito Benito en tejas todavÃÂ­a ahÃÂ­ s...  Content Creator   \n",
       "2  Here's mine. https://t.co/CI9DCGH8fkWhat did y...  Content Creator   \n",
       "3  Looking forward to the #NextAtAcer Global Pres...  Content Creator   \n",
       "4  @ironmouse @BubiVT Honestly true lmfao@_cherry...  Content Creator   \n",
       "5  . #RiotGrandPrix is back. Right now - see who ...  Content Creator   \n",
       "6  OG Leaking Flash Back https://t.co/ujDj47AHsn@...  Content Creator   \n",
       "7  @ItzNefarious Yes! \\n\\nÃ°ÂÂÂ https://t.co/u...  Content Creator   \n",
       "8  @joinsideplus yeetEL SIDEMEN Ã°ÂÂ«Â¡ https://...  Content Creator   \n",
       "9  New EXXXOTICA Blog! Breaking Into The Biz Ã¢Â...  Content Creator   \n",
       "\n",
       "   Encoded_label                              Processed description  \\\n",
       "0              1  content creator instagramcompokimanelol contac...   \n",
       "1              1  partner spanglish streamer anim food videogam ...   \n",
       "2              1  valor content creator influenc valor datamin i...   \n",
       "3              1   content creator #tech #travel #meme podcast host   \n",
       "4              1  horribl creatur mama banner #nyanart content c...   \n",
       "5              1  north american influenc team support content c...   \n",
       "6              1  content creator new report content creator #st...   \n",
       "7              1  stream record creat empow content creator sinc...   \n",
       "8              1  content creator busi contact vikbarncom instag...   \n",
       "9              1  largest event dedic love wildli illprepar onli...   \n",
       "\n",
       "                                    Processed tweets  \n",
       "0  absolut cutiest patootiest stream beta year we...  \n",
       "1                   benito teja todava hello qweenlt  \n",
       "2  here mine httpstcocidcghfkwhat your night mark...  \n",
       "3  look forward #nextatac global press confer liv...  \n",
       "4  honestli true lmfao thank much thi comm seriou...  \n",
       "5  #riotgrandprix back right win bracket master d...  \n",
       "6  leak flash back httpstcoujdjahsn want fortnit ...  \n",
       "7  httpstcouaejpsmye httpstcoklifmleei enhanc her...  \n",
       "8  yeetel sidemen httpstcoonshewhgleav like https...  \n",
       "9  exxxotica blog break into howto guid get into ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['description', 'tweets']\n",
    "for column in columns:\n",
    "    def remove_pattern(column_data, pattern):\n",
    "        processed_data = re.sub(pattern,\"\", column_data)\n",
    "        return processed_data\n",
    "    \n",
    "    #Removing twitter handles\n",
    "    sample_data[\"Processed \"+column] = np.vectorize(remove_pattern)(sample_data[column], \"@[\\w]*\")\n",
    "    \n",
    "    #removing punctuations\n",
    "    sample_data[\"Processed \"+column] = sample_data[\"Processed \"+column].str.replace(\"[^a-zA-Z#\\s]\", \"\")\n",
    "\n",
    "    #Removing short words\n",
    "    sample_data[\"Processed \"+column] = sample_data[\"Processed \"+column].apply(\n",
    "    lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "\n",
    "    #Tokenization\n",
    "    Tokenizer = TweetTokenizer()\n",
    "    sample_data[\"Processed \"+column] = sample_data[\"Processed \"+column].apply(lambda x: Tokenizer.tokenize(str(x)))\n",
    "\n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    sample_data[\"Processed \"+column] = sample_data[\"Processed \"+column].apply(lambda string: [ps.stem(letter) for letter in string])\n",
    "    \n",
    "    #Stiching the tokens back\n",
    "    for i in range(len(sample_data[\"Processed \"+column])):\n",
    "        sample_data[\"Processed \"+column][i] = ' '.join(sample_data[\"Processed \"+column][i])\n",
    "sample_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed description</th>\n",
       "      <th>Processed tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content creator instagramcompokimanelol contac...</td>\n",
       "      <td>absolut cutiest patootiest stream beta year we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>partner spanglish streamer anim food videogam ...</td>\n",
       "      <td>benito teja todava hello qweenlt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valor content creator influenc valor datamin i...</td>\n",
       "      <td>here mine httpstcocidcghfkwhat your night mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content creator #tech #travel #meme podcast host</td>\n",
       "      <td>look forward #nextatac global press confer liv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horribl creatur mama banner #nyanart content c...</td>\n",
       "      <td>honestli true lmfao thank much thi comm seriou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>tenista tenni player facebookcomgustavokuerten...</td>\n",
       "      <td>tamo chegando vamooo httpstcomulpmkngjonova ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>tenni player instagram contacto juancom</td>\n",
       "      <td>httpstcodhpagsfr httpstcokedoekbi httpstcoegig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>tenni player</td>\n",
       "      <td>httpstcolgiuhcaypwno palabra httpstcozkmexvcha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>tenni player olymp gold medalist silver medali...</td>\n",
       "      <td>httpstcoutjywmse soon httpstcofbmtnvmaf httpst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>profession tenni player world tour</td>\n",
       "      <td>httpstcokbpufvuisprt pour jouer plu #birthday ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Processed description  \\\n",
       "0     content creator instagramcompokimanelol contac...   \n",
       "1     partner spanglish streamer anim food videogam ...   \n",
       "2     valor content creator influenc valor datamin i...   \n",
       "3      content creator #tech #travel #meme podcast host   \n",
       "4     horribl creatur mama banner #nyanart content c...   \n",
       "...                                                 ...   \n",
       "5935  tenista tenni player facebookcomgustavokuerten...   \n",
       "5936            tenni player instagram contacto juancom   \n",
       "5937                                       tenni player   \n",
       "5938  tenni player olymp gold medalist silver medali...   \n",
       "5939                 profession tenni player world tour   \n",
       "\n",
       "                                       Processed tweets  \n",
       "0     absolut cutiest patootiest stream beta year we...  \n",
       "1                      benito teja todava hello qweenlt  \n",
       "2     here mine httpstcocidcghfkwhat your night mark...  \n",
       "3     look forward #nextatac global press confer liv...  \n",
       "4     honestli true lmfao thank much thi comm seriou...  \n",
       "...                                                 ...  \n",
       "5935  tamo chegando vamooo httpstcomulpmkngjonova ca...  \n",
       "5936  httpstcodhpagsfr httpstcokedoekbi httpstcoegig...  \n",
       "5937  httpstcolgiuhcaypwno palabra httpstcozkmexvcha...  \n",
       "5938  httpstcoutjywmse soon httpstcofbmtnvmaf httpst...  \n",
       "5939  httpstcokbpufvuisprt pour jouer plu #birthday ...  \n",
       "\n",
       "[5940 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sample_data.iloc[:, 4:]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "5935    5\n",
       "5936    5\n",
       "5937    5\n",
       "5938    5\n",
       "5939    5\n",
       "Name: Encoded_label, Length: 5940, dtype: int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = sample_data.iloc[:, 3]\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach by using training BOW and TF-IDF on two separate columns and then stacking the results of both "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-word Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectorizer = CountVectorizer(max_df=0.90,\n",
    "                                   min_df=2,\n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')\n",
    "tweets_vectorizer = CountVectorizer(max_df=0.90,\n",
    "                                   min_df=2,\n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = description_vectorizer.fit_transform(X['Processed description'])\n",
    "tweets_vectors = tweets_vectorizer.fit_transform(X['Processed tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5940x2000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 210170 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_2 = sp.hstack([description_vectors, tweets_vectors], format='csr')\n",
    "combined_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5940 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9     ...  1990  \\\n",
       "0        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "5935     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5936     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5937     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5938     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "5939     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "      1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0        0     1     0     0     0     0     0     0     0  \n",
       "1        0     0     0     0     0     0     0     0     0  \n",
       "2        0     0     0     0     0     0     0     0     0  \n",
       "3        0     0     0     0     0     0     0     0     0  \n",
       "4        0     0     0     0     0     0     0     0     0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "5935     0     0     0     0     0     0     0     0     0  \n",
       "5936     0     0     0     0     0     0     0     0     0  \n",
       "5937     0     0     0     0     0     0     0     0     0  \n",
       "5938     0     1     0     0     0     0     0     0     0  \n",
       "5939     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5940 rows x 2000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(combined_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow = combined_2\n",
    "train_bow.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,Y,test_size=0.3,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to save, load model and make predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(classifier, model_name):\n",
    "    pickle.dump(classifier, open(model_name+'_trained_model.pkl', 'wb'))\n",
    "def load_model(model_name):\n",
    "    model = pickle.load(open(model_name+'_trained_model.pkl', 'rb'))\n",
    "    return model\n",
    "def predictions(model, x_valid, y_valid):\n",
    "    model_predictions = model.predict(x_valid)\n",
    "    model_prediction_df = pd.DataFrame(y_valid)\n",
    "    model_prediction_df['Predicted'] = model_predictions\n",
    "    evaluation(model_prediction_df[\"Encoded_label\"],model_prediction_df[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ytest, pred):\n",
    "    accuracy = accuracy_score(ytest, pred)\n",
    "    precision = precision_score(ytest, pred, average='macro')\n",
    "    recall = recall_score(ytest, pred, average='macro')\n",
    "    f1score = f1_score(ytest, pred, average='macro')\n",
    "    precision = \"{:.1%}\".format(precision) \n",
    "    b= PrettyTable() \n",
    "    b.field_names = [\"Average_Type\",\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"]\n",
    "    b.add_row([\"Macro\",round(accuracy,3), precision ,round(recall,3),round(f1score,3)])\n",
    "    precision = precision_score(ytest, pred, average='weighted')\n",
    "    recall = recall_score(ytest, pred, average='weighted')\n",
    "    f1score = f1_score(ytest, pred, average='weighted')\n",
    "    precision = \"{:.1%}\".format(precision) \n",
    "    b.add_row([\"Weighted\",round(accuracy,3), precision ,round(recall,3),round(f1score,3)])\n",
    "    print(\"\\n\\nEvaluation Scores:\")\n",
    "    print(\"==================\\n\")\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model Training with BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_train_bow.toarray(),y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.557   |   55.5%   | 0.553  |  0.544   |\n",
      "|   Weighted   |  0.557   |   55.7%   | 0.557  |  0.547   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(nb_model, 'nb')\n",
    "model = load_model(\"nb\")\n",
    "predictions(model, x_valid_bow.toarray(), y_valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model Training with BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = svm.SVC()\n",
    "svc_model.fit(x_train_bow,y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.815   |   81.8%   | 0.814  |  0.814   |\n",
      "|   Weighted   |  0.815   |   82.0%   | 0.815  |  0.816   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(svc_model, 'svc')\n",
    "model = load_model(\"svc\")\n",
    "predictions(model, x_valid_bow, y_valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Training with BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression() \n",
    "lr_model.fit(x_train_bow, y_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.858   |   85.8%   | 0.857  |  0.857   |\n",
      "|   Weighted   |  0.858   |   86.0%   | 0.858  |  0.859   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(lr_model, 'lr')\n",
    "model = load_model(\"lr\")\n",
    "predictions(model, x_valid_bow, y_valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Training with BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "RF_model = RandomForestClassifier()\n",
    "RF_model = RF_model.fit(x_train_bow,y_train_bow)\n",
    "print(RF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.882   |   88.1%   | 0.881  |  0.881   |\n",
      "|   Weighted   |  0.882   |   88.2%   | 0.882  |  0.882   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(RF_model, 'RF')\n",
    "model = load_model(\"RF\")\n",
    "predictions(model, x_valid_bow, y_valid_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF followed bt bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tfidf_vectorizer = TfidfVectorizer(max_df=0.90,\n",
    "                                   min_df=2,\n",
    "                                   stop_words='english')\n",
    "tweets_tfidf_vectorizer = TfidfVectorizer(max_df=0.90,\n",
    "                                   min_df=2,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tfidf_vectors = description_tfidf_vectorizer.fit_transform(X['Processed description'])\n",
    "tweets_tfidf_vectors = tweets_tfidf_vectorizer.fit_transform(X['Processed tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5940x20712 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 325191 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tfidf = sp.hstack([description_tfidf_vectors, tweets_tfidf_vectors], format='csr')\n",
    "combined_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_matrix = combined_tfidf\n",
    "train_tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,Y,test_size=0.3,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1782x20712 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 98555 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Model Training using CountVectorizer followed by TfidfTransformer\n",
    " - This is done by using a scikit-learn function named as TfidfVectorizer() that apply both of them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tfidf_model = svm.SVC()\n",
    "svc_tfidf_model.fit(x_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.897   |   89.8%   | 0.897  |  0.897   |\n",
      "|   Weighted   |  0.897   |   89.9%   | 0.897  |  0.898   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(svc_tfidf_model, 'svc_tfidf')\n",
    "model = load_model(\"svc_tfidf\")\n",
    "predictions(model, x_valid_tfidf, y_valid_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Training using CountVectorizer followed by TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf_model = LogisticRegression() \n",
    "lr_tfidf_model.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.899   |   89.8%   | 0.898  |  0.898   |\n",
      "|   Weighted   |  0.899   |   89.9%   | 0.899  |  0.899   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(lr_tfidf_model, 'lr_tfidf')\n",
    "model = load_model(\"lr_tfidf\")\n",
    "predictions(model, x_valid_tfidf, y_valid_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Model Training using CountVectorizer followed by TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n"
     ]
    }
   ],
   "source": [
    "RF_tfidf_model = RandomForestClassifier()\n",
    "RF_tfidf_model = RF_model.fit(x_train_tfidf,y_train_tfidf)\n",
    "print(RF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluation Scores:\n",
      "==================\n",
      "\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "| Average_Type | Accuracy | Precision | Recall | F1 score |\n",
      "+--------------+----------+-----------+--------+----------+\n",
      "|    Macro     |  0.868   |   86.6%   | 0.866  |  0.865   |\n",
      "|   Weighted   |  0.868   |   86.7%   | 0.868  |  0.867   |\n",
      "+--------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "save_model(RF_model, 'RF_tfidf')\n",
    "model = load_model(\"RF_tfidf\")\n",
    "predictions(model, x_valid_tfidf, y_valid_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model using BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras import layers\n",
    "\n",
    "# input_dim = x_train_bow.shape[1]  # Number of features\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN link\n",
    "- https://realpython.com/python-keras-text-classification/#choosing-a-data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}